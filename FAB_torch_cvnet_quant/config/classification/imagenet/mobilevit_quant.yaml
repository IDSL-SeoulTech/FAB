taskname: '+ MobileViTv1-xxSmall'
common:
  run_label: "train"
  log_freq: 100
  auto_resume: true
  mixed_precision: false
dataset:
  root_train: /home/sonic/data/imagenet/train  # Please update the location of training set
  root_val: /home/sonic/data/imagenet/val # Please update the location of validation set
  name: "imagenet"
  category: "classification"
  train_batch_size0: 256 # In our experiments, we used an effective batch size of 1024 (128 images/GPU * 8 GPUs)
  val_batch_size0: 64
  eval_batch_size0: 100
  workers: 8
  persistent_workers: true
  pin_memory: true
image_augmentation:
  random_resized_crop:
    enable: true
    interpolation: "bilinear"
  random_horizontal_flip:
    enable: true
  resize:
    enable: true
    size: 288 # shorter size is 256
    interpolation: "bilinear"
  center_crop:
    enable: true
    size: 256
sampler:
  name: "variable_batch_sampler"
  vbs:
    crop_size_width: 256
    crop_size_height: 256
    max_n_scales: 5
    min_crop_size_width: 160
    max_crop_size_width: 320
    min_crop_size_height: 160
    max_crop_size_height: 320
    check_scale: 32
loss:
  category: "classification"
  classification:
    name: "cross_entropy"
    cross_entropy:
      label_smoothing: 0.1
optim:
  name: "adamw"
  weight_decay: 0.01
  no_decay_bn_filter_bias: false
  adamw:
    beta1: 0.9
    beta2: 0.999
scheduler:
  name: "cosine"
  is_iteration_based: true #false
  max_epochs: 300
  warmup_iterations: 20000 # longer warm-up
  warmup_init_lr: 0.000002 #qat_w_max_calib lr 0.0001 
  cosine:
    max_lr: 0.00002  #qat_w_max_calib lr  0.00001 
    min_lr: 0.000002  #qat_w_max_calib lr 0.000001 
model:
  classification:
    name: "mobilevit"
    classifier_dropout: 0.1
    mit:
      mode: "xx_small"
      ffn_dropout: 0.0
      attn_dropout: 0.0
      dropout: 0.0
      number_heads: 4
      no_fuse_local_global_features: false
      conv_kernel_size: 3
    activation:
      name: "swish"
  ignore_missing_scopes: [.*scale$,.*zero_point$, .*int_bias$, .*int_weight$,.*bias_scale$,.*max_value$,.*alpha$,.*olc_add$]
  normalization:
    name: "batch_norm"
    momentum: 0.1
  activation:
    name: "swish"
  layer:
    global_pool: "mean"
    conv_init: "kaiming_normal"
    linear_init: "trunc_normal"
    linear_init_std_dev: 0.02
ema:
  enable: true
  momentum: 0.0005
stats:
  val: [ "loss", "top1", "top5" ]
  train: ["loss"]
  checkpoint_metric: "top1"
  checkpoint_metric_max: true
quant:
  quant: true
  quant_method: "int8"
  weight_bit : "int8"
  activation_bit : "int8"
  calibration_a: "layer_wise"
  calibration_w: "layer_wise"
  calibration_c: "channel_wise"
  calib_iter: 1
  qat: true
  qat_weight_dir:  
  
  
